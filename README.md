# Optimization

* Sizi minimum maliyete götürmek için kullanabileceğiniz birçok farklı optimizasyon algoritması vardır. Benzer şekilde, bu tepeden en alçak noktaya birçok farklı patika vardır.

![cost](https://user-images.githubusercontent.com/54184905/89727117-aaf6bd00-da2a-11ea-96dd-41a5c2231bdf.jpg)


# Adımlar

* Gradient Descent (Dereceli alçalma)

![Screenshot_2020-08-09_10-30-22](https://user-images.githubusercontent.com/54184905/89727275-b4ccf000-da2b-11ea-90ac-466e20896712.png)

* Mini-Batch Gradient descent (Mini Toplu Gradyan inişi)

![Screenshot_2020-08-09_10-35-09](https://user-images.githubusercontent.com/54184905/89727318-13926980-da2c-11ea-8fe5-c704e09b5a5b.png)

* Momentum

![Screenshot_2020-08-09_10-29-41](https://user-images.githubusercontent.com/54184905/89727276-b5fe1d00-da2b-11ea-83e8-cc9160a23ade.png)

* Adam Optimization

![Screenshot_2020-08-09_10-30-39](https://user-images.githubusercontent.com/54184905/89727277-b696b380-da2b-11ea-9c89-444507c71303.png)


# Çıkarımlar

* Adam ve RMS prop arasındaki sezgiyi anlamak

* Mini toplu gradyan inişinin önemini öğrenmek

* Momentumun modelinizin genel performansı üzerindeki etkilerini görmek

![Screenshot_2020-08-09_10-32-15](https://user-images.githubusercontent.com/54184905/89727279-b72f4a00-da2b-11ea-9a00-b546e00e6b06.png)

* Kaynak : Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization (Coursera) - https://www.coursera.org/learn/deep-neural-network
